{
 "metadata": {
  "language": "lua",
  "name": "",
  "signature": "sha256:abe2838e1c22e8b76f79116f677413d50bc3994058471dc70290e73a58b729e5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load required packages and data, created in iPython Notebook file \"Create Data\" in this directory"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "require 'nn';\n",
      "require 'hdf5';\n",
      "require 'gnuplot';\n",
      "require 'optim';"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "myFile = hdf5.open('movies.hdf5', 'r')\n",
      "data = myFile:read('/'):all()\n",
      "\n",
      "train = data['train']\n",
      "target = data['target']\n",
      "test = data['test']\n",
      "test_target = data['test_target']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We choose d = 20, that is, each word has 20 dimensions in our embedding layer. Aditionally, there are two classes (good and bad). We use the length of the vocab + 1 because we have an additional term for padding (1) "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "V = 18765 + 1 -- size of vocab\n",
      "d = 20 -- size of feature representation\n",
      "nY = 2 -- 2 classes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We create the following model:\n",
      "$$p_i = \\sigma\\left(\\boldsymbol \\beta^T\\left(\\tanh\\left(\\sum_{w=1}^{\\text{sentence length}}\\boldsymbol E\\boldsymbol w_i\\right)\\right) + \\beta_0\\right)$$\n",
      "In the model, each word is one-hot coded in $\\boldsymbol w_i$, and thus the lookup table, $\\boldsymbol E$, is $V \\times d$. (No need to specify that it's one-hot coded to Torch). We add a tanh activation function, a linear term (the $\\boldsymbol \\beta$), and finally the log soft max."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = nn.Sequential()\n",
      "lookup = nn.LookupTable(V, d)\n",
      "model:add(lookup)\n",
      "model:add(nn.Sum(2))\n",
      "model:add(nn.Tanh())\n",
      "betas = nn.Linear(d, 2)\n",
      "model:add(betas)\n",
      "model:add(nn.LogSoftMax())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We use the negative log likelihood class criterion. Note that this criterion takes a log probability, which is why we use a log softmax above and not a regular softmax"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "criterion = nn.ClassNLLCriterion()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function validate(_epoch,dataset)\n",
      "    output = model:forward(dataset)\n",
      "    prob, indices = torch.sort(output, true)\n",
      "    return indices[{{}, 1}]\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are our hyperparameters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "batch_size = 5\n",
      "learning_rate = 1\n",
      "num_epochs = 80\n",
      "confusion = optim.ConfusionMatrix({1, 2})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We run stochastic gradient descent for every epoch. The train:narrow and target:narrow functions subset the data to the right size, and by updating lookup.weight[1]:zero(), we're making sure the padding parameter always has weight 0. Finally, we're evaluating the testset for every epoch"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for epoch = 1,num_epochs do\n",
      "    nll = 0\n",
      "    totalsize = 0\n",
      "    for t = 1, train:size()[1], batch_size do\n",
      "\n",
      "        model:zeroGradParameters()\n",
      "        \n",
      "        inputs = train:narrow(1,t,math.min(train:size()[1]-t, batch_size))\n",
      "        targets = target:narrow(1,t,math.min(train:size()[1]-t, batch_size))\n",
      "\n",
      "        --Forward Pass\n",
      "        out = model:forward(inputs)\n",
      "        nll = nll + batch_size*criterion:forward(out, targets)\n",
      "\n",
      "        -- Backward Pass\n",
      "        deriv = criterion:backward(out, targets)\n",
      "        model:backward(inputs, deriv)\n",
      "        model:updateParameters(learning_rate)\n",
      "        lookup.weight[1]:zero()\n",
      "        totalsize = totalsize + batch_size\n",
      "    end\n",
      "    --print(\"Epoch:\",epoch,nll)\n",
      "    --print(\"Epoch:\",epoch,math.exp(-nll/totalsize))\n",
      "    confusion:zero()\n",
      "    prediction = validate(epoch,test)\n",
      "    for t = 1, (#test)[1] do\n",
      "        confusion:add(prediction[t], test_target[t])\n",
      "    end\n",
      "end\n",
      "print(confusion)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "ConfusionMatrix:\n",
        "[[     406     156]   72.242% \t[class: 1]\n",
        " [     170     381]]  69.147% \t[class: 2]\n",
        " + average row correct: 70.694497227669% \n",
        " + average rowUcol correct (VOC measure): 54.677078127861% \n",
        " + global correct: 70.709793351303%\n",
        "{\n",
        "  valids : FloatTensor - size: 2\n",
        "  mat : LongTensor - size: 2x2\n",
        "  averageUnionValid : 0.54677078127861\n",
        "  _targ_idx : LongTensor - empty\n",
        "  averageValid : 0.70694497227669\n",
        "  classes : \n",
        "    {\n",
        "      1 : 1\n",
        "      2 : 2\n",
        "    }\n",
        "  _prediction : FloatTensor - empty\n",
        "  _pred_idx : LongTensor - empty\n",
        "  nclasses : 2\n",
        "  _max : FloatTensor - empty\n",
        "  _target : FloatTensor - empty\n",
        "  unionvalids : FloatTensor - size: 2\n",
        "  totalValid : 0.70709793351303\n",
        "}\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we write the weights to an hdf5 file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file = hdf5.open(\"movieweights.h5\",\"w\")\n",
      "file:write(\"lookup\", lookup.weight)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file = hdf5.open(\"movieweights2.h5\",\"w\")\n",
      "file:write(\"lookup2\", lookup.weight)\n",
      "file:write(\"betas\",betas.weight)\n",
      "file:close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "betas.weight"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "Columns 1 to 10\n",
        "-2.2885 -1.8833 -2.3269  1.6404  0.4700  2.1960  2.0462 -2.1939 -2.5468 -2.4276\n",
        " 2.9747  2.0545  2.1652 -1.3190 -0.8286 -1.7801 -1.6484  1.6397  2.7410  2.5165\n",
        "\n",
        "Columns 11 to 20\n",
        "-1.4130  0.8183 -1.2968  2.5084  1.0087 -0.8592 -2.9840 -2.4676  2.2832 -0.0169\n",
        " 1.2810 -0.6391  1.3069 -3.4932 -1.0365  0.5965  3.9407  2.2872 -2.4202 -0.0937\n",
        "[torch.DoubleTensor of size 2x20]\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ignore this -- this is some clunky code to validate our test set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = torch.LongTensor(1,61)\n",
      "correct = 0\n",
      "n = 8000\n",
      "for i = 1,n do\n",
      "    x[1] = train[i]\n",
      "    out = model:forward(x)\n",
      "    if out[1][1] > out[1][2] then\n",
      "        if target[i] == 1 then\n",
      "            correct = correct + 1\n",
      "        end\n",
      "    else\n",
      "        if target[i] == 2 then\n",
      "            correct = correct + 1\n",
      "        end\n",
      "    end\n",
      "end\n",
      "print(correct/n)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 85,
       "text": [
        "0.513375\t\n"
       ]
      }
     ],
     "prompt_number": 85
    }
   ],
   "metadata": {}
  }
 ]
}